{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76250373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torchtext.data import TabularDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410c36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ro_nlp = spacy.load(\"ro_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db84e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in en_nlp(text)]\n",
    "\n",
    "def tokenize_ro(text):\n",
    "    return [tok.text for tok in ro_nlp(text)]\n",
    "\n",
    "source_language = Field(tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "target_language = Field(tokenize_ro, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "fields = [('english', source_language), ('romanian', target_language)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e484a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12145, Val: 1518, Test: 1519\n",
      "Sample data:\n",
      "Train sample: ['he', 'came', 'at', 'about', 'two', \"o'clock.\"] ['el', 'a', 'venit', 'aproximativ', 'pe', 'la', 'două.']\n",
      "Validation sample: ['no', \"i'm\", 'not;', 'you', 'are!'] ['nu,', 'nu', 'sunt;', 'tu', 'ești!']\n",
      "Test sample: ['i', 'think', 'i', 'like', 'eating', 'white', 'rice', 'better', 'than', 'brown', 'rice.'] ['cred', 'că-mi', 'place', 'mai', 'mult', 'să', 'mănânc', 'orez', 'alb', 'decât', 'orez', 'brun.']\n"
     ]
    }
   ],
   "source": [
    "# dataset =load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"ro\")\n",
    "# data = [(item['translation']['en'], item['translation']['ro']) for item in dataset['train']]\n",
    "# train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "train_data_path = \"data/train_data.csv\"\n",
    "val_data_path = \"data/val_data.csv\"\n",
    "test_data_path = \"data/test_data.csv\"\n",
    "\n",
    "\n",
    "train_data = TabularDataset(\n",
    "    path=train_data_path, format='csv',\n",
    "    fields=fields, skip_header=True \n",
    ")\n",
    "\n",
    "val_data = TabularDataset(\n",
    "    path=val_data_path, format='csv',\n",
    "    fields=fields, skip_header=True\n",
    ")\n",
    "\n",
    "test_data = TabularDataset(\n",
    "    path=test_data_path, format='csv',\n",
    "    fields=fields, skip_header=True\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "print(\"Sample data:\")\n",
    "print(\"Train sample:\", train_data.examples[0].english, train_data.examples[0].romanian)\n",
    "print(\"Validation sample:\", val_data.examples[0].english, val_data.examples[0].romanian)\n",
    "print(\"Test sample:\", test_data.examples[0].english, test_data.examples[0].romanian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f40b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 4752\n",
      "Target vocabulary size: 5450\n",
      "Source vocabulary sample: [('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('the', 4), ('i', 5), ('to', 6), ('a', 7), ('is', 8), ('you', 9)]\n",
      "Target vocabulary sample: [('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('de', 4), ('să', 5), ('nu', 6), ('a', 7), ('este', 8), ('tom', 9)]\n"
     ]
    }
   ],
   "source": [
    "source_language.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "target_language.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "print(f\"Source vocabulary size: {len(source_language.vocab)}\")\n",
    "print(f\"Target vocabulary size: {len(target_language.vocab)}\")\n",
    "\n",
    "# show some vocabulary\n",
    "print(\"Source vocabulary sample:\", list(source_language.vocab.stoi.items())[:10])\n",
    "print(\"Target vocabulary sample:\", list(target_language.vocab.stoi.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f54c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #shape (x) = (seq_len, batch_size)\n",
    "        embedding =  self.dropout(self.embedding(x))\n",
    "        #shape (embedding) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        _, (hidden, cell) = self.lstm(embedding)\n",
    "         \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        #shape (x) = (N)\n",
    "        x= x.unsqueeze(0) # Add sequence dimension (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        #shape (embedding) = (1, N, embedding_size)\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        #shape (outputs) = (1, N, hidden_size)\n",
    "        predictions = self.fc(outputs)\n",
    "        #shape (predictions) = (1, N, output_size)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        #shape (predictions) = (N, output_size)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio = 0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        # shape (source) = (seq_len, batch_size)\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(target_language.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        # hidden, cell = self.encoder(source)\n",
    "        (hidden, cell), encoder_outputs = self.encoder(source)\n",
    "\n",
    "        # first token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell, encoder_outputs)\n",
    "            # shape (output) = (batch_size, target_vocab_size)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # output shape (output) = (batch_size, target_vocab_size)\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if torch.rand(1) < teacher_force_ratio else best_guess\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "class CnnEncoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, encoder_num_layers, dropout_rate, decoder_num_layers, max_seq_len=100, kernel_size=3):\n",
    "        super(CnnEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder_num_layers = encoder_num_layers\n",
    "        self.decoder_num_layers = decoder_num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, embedding_size)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_size if i == 0 else hidden_size,\n",
    "                       out_channels=hidden_size, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "            for i in range(encoder_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size, decoder_num_layers * hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size, decoder_num_layers * hidden_size)\n",
    "    def forward(self, x):\n",
    "        # shape (x) = (seq_len, batch_size)\n",
    "        embeddings = self.dropout(self.embedding(x))\n",
    "        # shape (embeddings) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        positions = torch.arange(0, x.size(0), device=x.device).unsqueeze(1).repeat(1, x.size(1))\n",
    "        pos_embeddings = self.dropout(self.pos_embedding(positions))\n",
    "        # shape (pos_embeddings) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        embeddings = embeddings + pos_embeddings\n",
    "        # shape (embeddings) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        embeddings = embeddings.permute(1, 2, 0)\n",
    "        # shape (embeddings) = (batch_size, embedding_size, seq_len)\n",
    "\n",
    "        conv_input = embeddings\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            conv_output = torch.relu(conv(conv_input))\n",
    "            # shape (conv_output) = (batch_size, hidden_size, seq_len)\n",
    "\n",
    "            #residual connection\n",
    "            if i > 0:\n",
    "                conv_output = self.dropout(conv_output) + conv_input\n",
    "            else:\n",
    "                conv_output = self.dropout(conv_output)\n",
    "            conv_input = conv_output\n",
    "            # shape (conv_output) = (batch_size, hidden_size, seq_len)\n",
    "        \n",
    "        '''This part is commented out because it was the original implementation, without attention.'''\n",
    "        # # get rid of the sequence dimension\n",
    "        # conv_output = conv_output.mean(dim=2)\n",
    "        # # shape (conv_output) = (batch_size, hidden_size)\n",
    "\n",
    "        # hidden = self.fc_hidden(conv_output)\n",
    "        # cell = self.fc_cell(conv_output)\n",
    "\n",
    "        # # reshape for lstm\n",
    "        # hidden = hidden.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "        # cell = cell.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "\n",
    "        conv_output_perm = conv_output.permute(2, 0, 1)\n",
    "        # shape (conv_output_perm) = (seq_len, batch_size, hidden_size)\n",
    "        conv_output_mean = conv_output_perm.mean(dim=0)\n",
    "        # shape (conv_output_mean) = (batch_size, hidden_size)\n",
    "\n",
    "        hidden = self.fc_hidden(conv_output_mean)\n",
    "        cell = self.fc_cell(conv_output_mean)\n",
    "\n",
    "        # reshape for lstm\n",
    "        hidden = hidden.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "        cell = cell.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "    \n",
    "        return (hidden, cell), conv_output_perm\n",
    "\n",
    "class Attention(nn.Module): # Luong concatenation attention \n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(encoder_hidden_size + decoder_hidden_size, decoder_hidden_size)\n",
    "        self.v = nn.Linear(decoder_hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [num_layers, batch_size, decoder_hidden_size]\n",
    "        # encoder_outputs: [src_len, batch_size, encoder_hidden_size]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        # hidden_last_layer shape: [1, batch_size, decoder_hidden_size]\n",
    "        hidden_last_layer = hidden[-1, :, :].unsqueeze(0) \n",
    "\n",
    "        # repeat hidden_last_layer for each step in the source sequence\n",
    "        # hidden_repeated shape: [src_len, batch_size, decoder_hidden_size]\n",
    "        hidden_repeated = hidden_last_layer.repeat(src_len, 1, 1)\n",
    "\n",
    "        # hidden_repeated + encoder_outputs\n",
    "        # combined shape: [src_len, batch_size, encoder_hidden_size + decoder_hidden_size]\n",
    "        combined = torch.cat((hidden_repeated, encoder_outputs), dim=2)\n",
    "\n",
    "        # calculate energy\n",
    "        # energy shape: [src_len, batch_size, decoder_hidden_size]\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "\n",
    "        # project energy vector to a single score\n",
    "        # attention shape: [src_len, batch_size, 1]\n",
    "        attention = self.v(energy)\n",
    "        \n",
    "        # attention shape: [src_len, batch_size]\n",
    "        attention = attention.squeeze(2)\n",
    "\n",
    "        # attention_weights shape: [src_len, batch_size]\n",
    "        attention_weights = F.softmax(attention, dim=0)\n",
    "\n",
    "        return attention_weights\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout_rate, encoder_hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_hidden_size = encoder_hidden_size \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
    "    \n",
    "        self.attention = Attention(encoder_hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size + encoder_hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x: (N) - tokenul curent (batch de tokenuri)\n",
    "        # hidden: (num_layers, N, hidden_size)\n",
    "        # cell: (num_layers, N, hidden_size)\n",
    "        # encoder_outputs: (src_len, N, encoder_hidden_size) \n",
    "\n",
    "        x = x.unsqueeze(0) # shape (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        # apply attention_weights to encoder_outputs to get context vector\n",
    "        # encoder_outputs shape: (src_len, N, encoder_hidden_size)\n",
    "        # attention_weights shape: (src_len, N)\n",
    "        # context_vector shape: (N, encoder_hidden_size)\n",
    "        context_vector = torch.sum(attention_weights.unsqueeze(2) * encoder_outputs, dim=0)\n",
    "\n",
    "        # combine the LSTM output and context vector\n",
    "        # outputs.squeeze(0) shape: (N, hidden_size)\n",
    "        # context_vector shape: (N, encoder_hidden_size)\n",
    "        combined_input = torch.cat((outputs.squeeze(0), context_vector), dim=1)\n",
    "        # combined_input shape: (N, hidden_size + encoder_hidden_size)\n",
    "\n",
    "        predictions = self.fc(combined_input)\n",
    "        # predictions shape: (N, output_size)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d2a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "load_model = False\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "input_size_encoder = len(source_language.vocab)\n",
    "input_size_decoder = len(target_language.vocab)\n",
    "output_size  = len(target_language.vocab)\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout_rate = 0.5\n",
    "dec_dropout_rate = 0.5\n",
    "cnn_encoder_layers = 5\n",
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.english),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# encoder = Encoder(input_size_encoder, embedding_size, hidden_size, num_layers, enc_dropout_rate).to(device)\n",
    "# decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dec_dropout_rate).to(device)\n",
    "cnn_encoder = CnnEncoder(input_size_encoder, embedding_size, hidden_size, cnn_encoder_layers, enc_dropout_rate, num_layers, max_seq_len).to(device)\n",
    "attn_decoder = AttentionDecoder(output_size, embedding_size, hidden_size, num_layers, dec_dropout_rate, hidden_size).to(device)\n",
    "model = Seq2Seq(cnn_encoder, attn_decoder).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pad_idx = target_language.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a5ba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iterator, val_iterator, optimizer, criterion, source_language, target_language, num_epochs):\n",
    "    wandb.init(project=\"nmt_training\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            source = batch.english\n",
    "            target = batch.romanian\n",
    "            source, target = source.to(device), target.to(device)\n",
    "\n",
    "            output = model(source, target)\n",
    "             # output shape (target_len, batch_size, output_size)\n",
    "            # target shape (target_len, batch_size)\n",
    "\n",
    "            #ignore the first token in target (sos)\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        valid_loss, valid_bleu = evaluate(model, val_iterator, criterion, source_language, target_language)\n",
    "        avg_train_loss = epoch_loss / len(train_iterator)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {avg_train_loss:.3f} | Val Loss: {valid_loss:.3f} | BLEU: {valid_bleu:.2f}')\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": valid_loss,\n",
    "            \"val_bleu\": valid_bleu,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "def evaluate(model, iterator, criterion, source_language, target_language):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            source = batch.english.to(device)\n",
    "            target = batch.romanian.to(device)\n",
    "\n",
    "            output = model(source, target, teacher_force_ratio=0.0)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # output: (target_len, batch_size, output_dim)\n",
    "            # target: (target_len, batch_size)\n",
    "            output_tokens = output.argmax(2)  # shape: (target_len, batch_size)\n",
    "\n",
    "            for i in range(target.shape[1]): \n",
    "                pred_tokens = output_tokens[1:, i]  # skip <sos>\n",
    "                trg_tokens = target[1:, i]\n",
    "\n",
    "                # Cut at <eos> \n",
    "                pred_sentence = []\n",
    "                for tok in pred_tokens:\n",
    "                    word = target_language.vocab.itos[tok.item()]\n",
    "                    if word == '<eos>':\n",
    "                        break\n",
    "                    pred_sentence.append(word)\n",
    "\n",
    "                trg_sentence = []\n",
    "                for tok in trg_tokens:\n",
    "                    word = target_language.vocab.itos[tok.item()]\n",
    "                    if word == '<eos>':\n",
    "                        break\n",
    "                    trg_sentence.append(word)\n",
    "\n",
    "                all_predictions.append(pred_sentence)\n",
    "                all_targets.append([trg_sentence])  # wrapped in list for BLEU\n",
    "\n",
    "            output_flat = output[1:].view(-1, output_dim)\n",
    "            target_flat = target[1:].view(-1)\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    bleu = bleu_score(all_predictions, all_targets) * 100\n",
    "    return epoch_loss / len(iterator), bleu\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1238b34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_bleu</td><td>▁▃▅▆▆▆▆▆▆█</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>2.60353</td></tr><tr><td>val_bleu</td><td>8.50428</td></tr><tr><td>val_loss</td><td>4.34718</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-disco-9</strong> at: <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/cgfcimxy' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/cgfcimxy</a><br> View project at: <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250525_210233-cgfcimxy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/denisangel2k2/Documents/Github/cnn-encoder/wandb/run-20250525_211641-viopz5wj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/viopz5wj' target=\"_blank\">ethereal-pond-10</a></strong> to <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/viopz5wj' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/viopz5wj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 2.451 | Val Loss: 4.362 | BLEU: 8.81\n",
      "Epoch: 02 | Train Loss: 2.310 | Val Loss: 4.441 | BLEU: 8.92\n",
      "Epoch: 03 | Train Loss: 2.194 | Val Loss: 4.465 | BLEU: 8.44\n",
      "Epoch: 04 | Train Loss: 2.081 | Val Loss: 4.474 | BLEU: 9.16\n",
      "Epoch: 05 | Train Loss: 1.996 | Val Loss: 4.489 | BLEU: 9.74\n",
      "Epoch: 06 | Train Loss: 1.900 | Val Loss: 4.540 | BLEU: 8.95\n",
      "Epoch: 07 | Train Loss: 1.806 | Val Loss: 4.625 | BLEU: 9.33\n",
      "Epoch: 08 | Train Loss: 1.743 | Val Loss: 4.624 | BLEU: 9.54\n",
      "Epoch: 09 | Train Loss: 1.672 | Val Loss: 4.617 | BLEU: 10.14\n",
      "Epoch: 10 | Train Loss: 1.639 | Val Loss: 4.600 | BLEU: 10.21\n",
      "Epoch: 11 | Train Loss: 1.562 | Val Loss: 4.674 | BLEU: 10.03\n",
      "Epoch: 12 | Train Loss: 1.572 | Val Loss: 4.584 | BLEU: 10.32\n",
      "Epoch: 13 | Train Loss: 1.498 | Val Loss: 4.651 | BLEU: 10.33\n",
      "Epoch: 14 | Train Loss: 1.452 | Val Loss: 4.691 | BLEU: 10.28\n",
      "Epoch: 15 | Train Loss: 1.402 | Val Loss: 4.721 | BLEU: 11.14\n",
      "Epoch: 16 | Train Loss: 1.385 | Val Loss: 4.725 | BLEU: 11.29\n",
      "Epoch: 17 | Train Loss: 1.328 | Val Loss: 4.787 | BLEU: 11.39\n",
      "Epoch: 18 | Train Loss: 1.297 | Val Loss: 4.790 | BLEU: 11.36\n",
      "Epoch: 19 | Train Loss: 1.261 | Val Loss: 4.800 | BLEU: 11.31\n",
      "Epoch: 20 | Train Loss: 1.244 | Val Loss: 4.858 | BLEU: 11.25\n",
      "Epoch: 21 | Train Loss: 1.228 | Val Loss: 4.818 | BLEU: 11.57\n",
      "Epoch: 22 | Train Loss: 1.195 | Val Loss: 4.861 | BLEU: 12.20\n",
      "Epoch: 23 | Train Loss: 1.149 | Val Loss: 4.907 | BLEU: 12.33\n",
      "Epoch: 24 | Train Loss: 1.146 | Val Loss: 4.912 | BLEU: 12.21\n",
      "Epoch: 25 | Train Loss: 1.166 | Val Loss: 4.903 | BLEU: 12.41\n",
      "Epoch: 26 | Train Loss: 1.159 | Val Loss: 4.857 | BLEU: 12.80\n",
      "Epoch: 27 | Train Loss: 1.086 | Val Loss: 4.907 | BLEU: 13.47\n",
      "Epoch: 28 | Train Loss: 1.064 | Val Loss: 4.916 | BLEU: 13.02\n",
      "Epoch: 29 | Train Loss: 1.067 | Val Loss: 4.951 | BLEU: 13.39\n",
      "Epoch: 30 | Train Loss: 1.014 | Val Loss: 4.962 | BLEU: 13.12\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_iterator, val_iterator, optimizer, criterion, source_language, target_language, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f342dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: el este foarte tristă.\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, model, source_field, target_field, device, max_len=50):\n",
    "    \"\"\"\n",
    "    Translates a single English sentence to Romanian using the trained Seq2Seq model with attention.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input English sentence to translate.\n",
    "        model (nn.Module): The trained Seq2Seq model.\n",
    "        source_field (torchtext.legacy.data.Field): The Field object for the source language.\n",
    "        target_field (torchtext.legacy.data.Field): The Field object for the target language.\n",
    "        device (torch.device): The device (CPU or GPU) the model is on.\n",
    "        max_len (int): The maximum length of the translated sentence to prevent infinite loops.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of translated tokens (words).\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    if source_field.lower:\n",
    "        tokens = [token.lower() for token in source_field.tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = source_field.tokenize(sentence)\n",
    "\n",
    "    tokens = [source_field.init_token] + tokens + [source_field.eos_token]\n",
    "    src_indexes = [source_field.vocab.stoi.get(token, source_field.vocab.stoi[source_field.unk_token]) for token in tokens]\n",
    "    \n",
    "    # Convert to PyTorch tensor and add batch dimension (batch_size=1)\n",
    "    # src_tensor shape: (seq_len, 1)\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # MODIFIED: Encoder now returns (hidden, cell) AND encoder_outputs\n",
    "        (hidden, cell), encoder_outputs = model.encoder(src_tensor)\n",
    "    \n",
    "    # Prepare the first input to the decoder: <sos> token\n",
    "    # trg_indexes will store the predicted token IDs\n",
    "    trg_indexes = [target_field.vocab.stoi[target_field.init_token]]\n",
    "    \n",
    "    # Decoding loop\n",
    "    for _ in range(max_len): # Use max_len parameter to prevent infinite loops\n",
    "        # trg_tensor shape: (1) - current input token for the decoder\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        # MODIFIED: Pass encoder_outputs to the decoder\n",
    "        output, hidden, cell = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n",
    "        \n",
    "        # Get the predicted next word (token with highest probability)\n",
    "        # output shape: (1, target_vocab_size) -> argmax(1) gives the index\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # If the model predicts an <eos> token, stop decoding\n",
    "        if pred_token == target_field.vocab.stoi[target_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    # Convert predicted IDs back to words\n",
    "    trg_tokens = [target_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    # Exclude <sos> token at the beginning and <eos> token at the end if present\n",
    "    if trg_tokens and trg_tokens[0] == target_field.init_token:\n",
    "        trg_tokens = trg_tokens[1:]\n",
    "    if trg_tokens and trg_tokens[-1] == target_field.eos_token:\n",
    "        trg_tokens = trg_tokens[:-1]\n",
    "\n",
    "    return trg_tokens\n",
    "\n",
    "translated_sentence = translate_sentence(\n",
    "    \"he is very strong\",\n",
    "    model,\n",
    "    source_language,\n",
    "    target_language,\n",
    "    device\n",
    ")\n",
    "print(\"Translated sentence:\", \" \".join(translated_sentence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
