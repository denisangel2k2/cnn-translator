{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "76250373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torchtext.data import TabularDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "410c36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ro_nlp = spacy.load(\"ro_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db84e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in en_nlp(text)]\n",
    "\n",
    "def tokenize_ro(text):\n",
    "    return [tok.text for tok in ro_nlp(text)]\n",
    "\n",
    "source_language = Field(tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "target_language = Field(tokenize_ro, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "fields = [('english', source_language), ('romanian', target_language)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e484a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12145, Val: 1518, Test: 1519\n",
      "Sample data:\n",
      "Train sample: ['he', 'came', 'at', 'about', 'two', \"o'clock.\"] ['el', 'a', 'venit', 'aproximativ', 'pe', 'la', 'două.']\n",
      "Validation sample: ['no', \"i'm\", 'not;', 'you', 'are!'] ['nu,', 'nu', 'sunt;', 'tu', 'ești!']\n",
      "Test sample: ['i', 'think', 'i', 'like', 'eating', 'white', 'rice', 'better', 'than', 'brown', 'rice.'] ['cred', 'că-mi', 'place', 'mai', 'mult', 'să', 'mănânc', 'orez', 'alb', 'decât', 'orez', 'brun.']\n"
     ]
    }
   ],
   "source": [
    "# dataset =load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"ro\")\n",
    "# data = [(item['translation']['en'], item['translation']['ro']) for item in dataset['train']]\n",
    "# train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "train_data_path = \"data/train_data.csv\"\n",
    "val_data_path = \"data/val_data.csv\"\n",
    "test_data_path = \"data/test_data.csv\"\n",
    "\n",
    "\n",
    "train_data = TabularDataset(\n",
    "    path=train_data_path, format='csv',\n",
    "    fields=fields, skip_header=True \n",
    ")\n",
    "\n",
    "val_data = TabularDataset(\n",
    "    path=val_data_path, format='csv',\n",
    "    fields=fields, skip_header=True\n",
    ")\n",
    "\n",
    "test_data = TabularDataset(\n",
    "    path=test_data_path, format='csv',\n",
    "    fields=fields, skip_header=True\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "print(\"Sample data:\")\n",
    "print(\"Train sample:\", train_data.examples[0].english, train_data.examples[0].romanian)\n",
    "print(\"Validation sample:\", val_data.examples[0].english, val_data.examples[0].romanian)\n",
    "print(\"Test sample:\", test_data.examples[0].english, test_data.examples[0].romanian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "82f40b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 4752\n",
      "Target vocabulary size: 5450\n",
      "Source vocabulary sample: [('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('the', 4), ('i', 5), ('to', 6), ('a', 7), ('is', 8), ('you', 9)]\n",
      "Target vocabulary sample: [('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('de', 4), ('să', 5), ('nu', 6), ('a', 7), ('este', 8), ('tom', 9)]\n"
     ]
    }
   ],
   "source": [
    "source_language.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "target_language.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "print(f\"Source vocabulary size: {len(source_language.vocab)}\")\n",
    "print(f\"Target vocabulary size: {len(target_language.vocab)}\")\n",
    "\n",
    "# show some vocabulary\n",
    "print(\"Source vocabulary sample:\", list(source_language.vocab.stoi.items())[:10])\n",
    "print(\"Target vocabulary sample:\", list(target_language.vocab.stoi.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b1f54c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #shape (x) = (seq_len, batch_size)\n",
    "        embedding =  self.dropout(self.embedding(x))\n",
    "        #shape (embedding) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        _, (hidden, cell) = self.lstm(embedding)\n",
    "         \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        #shape (x) = (N)\n",
    "        x= x.unsqueeze(0) # Add sequence dimension (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        #shape (embedding) = (1, N, embedding_size)\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        #shape (outputs) = (1, N, hidden_size)\n",
    "        predictions = self.fc(outputs)\n",
    "        #shape (predictions) = (1, N, output_size)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        #shape (predictions) = (N, output_size)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio = 0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        # shape (source) = (seq_len, batch_size)\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(target_language.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # first token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            # shape (output) = (batch_size, target_vocab_size)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # output shape (output) = (batch_size, target_vocab_size)\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if torch.rand(1) < teacher_force_ratio else best_guess\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "class CnnEncoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, encoder_num_layers, dropout_rate, decoder_num_layers, max_seq_len=100, kernel_size=3):\n",
    "        super(CnnEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder_num_layers = encoder_num_layers\n",
    "        self.decoder_num_layers = decoder_num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, embedding_size)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_size if i == 0 else hidden_size,\n",
    "                       out_channels=hidden_size, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "            for i in range(encoder_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size, decoder_num_layers * hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size, decoder_num_layers * hidden_size)\n",
    "    def forward(self, x):\n",
    "        # shape (x) = (seq_len, batch_size)\n",
    "        embeddings = self.dropout(self.embedding(x))\n",
    "        # shape (embeddings) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        positions = torch.arange(0, x.size(0), device=x.device).unsqueeze(1).repeat(1, x.size(1))\n",
    "        pos_embeddings = self.dropout(self.pos_embedding(positions))\n",
    "        # shape (pos_embeddings) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        embeddings = embeddings + pos_embeddings\n",
    "        # shape (embeddings) = (seq_len, batch_size, embedding_size)\n",
    "\n",
    "        embeddings = embeddings.permute(1, 2, 0)\n",
    "        # shape (embeddings) = (batch_size, embedding_size, seq_len)\n",
    "\n",
    "        conv_input = embeddings\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            conv_output = torch.relu(conv(conv_input))\n",
    "            # shape (conv_output) = (batch_size, hidden_size, seq_len)\n",
    "\n",
    "            #residual connection\n",
    "            if i > 0:\n",
    "                conv_output = self.dropout(conv_output) + conv_input\n",
    "            else:\n",
    "                conv_output = self.dropout(conv_output)\n",
    "            conv_input = conv_output\n",
    "            # shape (conv_output) = (batch_size, hidden_size, seq_len)\n",
    "        \n",
    "        '''This part is commented out because it was the original implementation, without attention.'''\n",
    "        # # get rid of the sequence dimension\n",
    "        # conv_output = conv_output.mean(dim=2)\n",
    "        # # shape (conv_output) = (batch_size, hidden_size)\n",
    "\n",
    "        # hidden = self.fc_hidden(conv_output)\n",
    "        # cell = self.fc_cell(conv_output)\n",
    "\n",
    "        # # reshape for lstm\n",
    "        # hidden = hidden.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "        # cell = cell.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "\n",
    "        conv_output_perm = conv_output.permute(2, 0, 1)\n",
    "        # shape (conv_output_perm) = (seq_len, batch_size, hidden_size)\n",
    "        conv_output_mean = conv_output_perm.mean(dim=0)\n",
    "        # shape (conv_output_mean) = (batch_size, hidden_size)\n",
    "\n",
    "        hidden = self.fc_hidden(conv_output_mean)\n",
    "        cell = self.fc_cell(conv_output_mean)\n",
    "\n",
    "        # reshape for lstm\n",
    "        hidden = hidden.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "        cell = cell.view(self.decoder_num_layers, -1, self.hidden_size)\n",
    "    \n",
    "        return (hidden, cell), conv_output_perm\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(encoder_hidden_size + decoder_hidden_size, decoder_hidden_size)\n",
    "        self.v = nn.Linear(decoder_hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [num_layers, batch_size, decoder_hidden_size]\n",
    "        # encoder_outputs: [src_len, batch_size, encoder_hidden_size]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        # hidden_last_layer shape: [1, batch_size, decoder_hidden_size]\n",
    "        hidden_last_layer = hidden[-1, :, :].unsqueeze(0) \n",
    "\n",
    "        # repeat hidden_last_layer for each step in the source sequence\n",
    "        # hidden_repeated shape: [src_len, batch_size, decoder_hidden_size]\n",
    "        hidden_repeated = hidden_last_layer.repeat(src_len, 1, 1)\n",
    "\n",
    "        # hidden_repeated + encoder_outputs\n",
    "        # combined shape: [src_len, batch_size, encoder_hidden_size + decoder_hidden_size]\n",
    "        combined = torch.cat((hidden_repeated, encoder_outputs), dim=2)\n",
    "\n",
    "        # calculate energy\n",
    "        # energy shape: [src_len, batch_size, decoder_hidden_size]\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "\n",
    "        # project energy vector to a single score\n",
    "        # attention shape: [src_len, batch_size, 1]\n",
    "        attention = self.v(energy)\n",
    "        \n",
    "        # attention shape: [src_len, batch_size]\n",
    "        attention = attention.squeeze(2)\n",
    "\n",
    "        # attention_weights shape: [src_len, batch_size]\n",
    "        attention_weights = F.softmax(attention, dim=0)\n",
    "\n",
    "        return attention_weights\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout_rate, encoder_hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_hidden_size = encoder_hidden_size \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
    "    \n",
    "        self.attention = Attention(encoder_hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size + encoder_hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x: (N) - tokenul curent (batch de tokenuri)\n",
    "        # hidden: (num_layers, N, hidden_size)\n",
    "        # cell: (num_layers, N, hidden_size)\n",
    "        # encoder_outputs: (src_len, N, encoder_hidden_size) \n",
    "\n",
    "        x = x.unsqueeze(0) # shape (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        # apply attention_weights to encoder_outputs to get context vector\n",
    "        # encoder_outputs shape: (src_len, N, encoder_hidden_size)\n",
    "        # attention_weights shape: (src_len, N)\n",
    "        # context_vector shape: (N, encoder_hidden_size)\n",
    "        context_vector = torch.sum(attention_weights.unsqueeze(2) * encoder_outputs, dim=0)\n",
    "\n",
    "        # combine the LSTM output and context vector\n",
    "        # outputs.squeeze(0) shape: (N, hidden_size)\n",
    "        # context_vector shape: (N, encoder_hidden_size)\n",
    "        combined_input = torch.cat((outputs.squeeze(0), context_vector), dim=1)\n",
    "        # combined_input shape: (N, hidden_size + encoder_hidden_size)\n",
    "\n",
    "        predictions = self.fc(combined_input)\n",
    "        # predictions shape: (N, output_size)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "02d2a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "load_model = False\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "input_size_encoder = len(source_language.vocab)\n",
    "input_size_decoder = len(target_language.vocab)\n",
    "output_size  = len(target_language.vocab)\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout_rate = 0.5\n",
    "dec_dropout_rate = 0.5\n",
    "cnn_encoder_layers = 5\n",
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.english),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder = Encoder(input_size_encoder, embedding_size, hidden_size, num_layers, enc_dropout_rate).to(device)\n",
    "decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dec_dropout_rate).to(device)\n",
    "cnn_encoder = CnnEncoder(input_size_encoder, embedding_size, hidden_size, cnn_encoder_layers, enc_dropout_rate, num_layers, max_seq_len).to(device)\n",
    "model = Seq2Seq(cnn_encoder, decoder).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pad_idx = target_language.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4a5ba44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iterator, val_iterator, optimizer, criterion, source_language, target_language, num_epochs):\n",
    "    wandb.init(project=\"nmt_training\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            source = batch.english\n",
    "            target = batch.romanian\n",
    "            source, target = source.to(device), target.to(device)\n",
    "\n",
    "            output = model(source, target)\n",
    "             # output shape (target_len, batch_size, output_size)\n",
    "            # target shape (target_len, batch_size)\n",
    "\n",
    "            #ignore the first token in target (sos)\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        valid_loss, valid_bleu = evaluate(model, val_iterator, criterion, source_language, target_language)\n",
    "        avg_train_loss = epoch_loss / len(train_iterator)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {avg_train_loss:.3f} | Val Loss: {valid_loss:.3f} | BLEU: {valid_bleu:.2f}')\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": valid_loss,\n",
    "            \"val_bleu\": valid_bleu,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "def evaluate(model, iterator, criterion, source_language, target_language):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            source = batch.english.to(device)\n",
    "            target = batch.romanian.to(device)\n",
    "\n",
    "            output = model(source, target, teacher_force_ratio=0.0)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # output: (target_len, batch_size, output_dim)\n",
    "            # target: (target_len, batch_size)\n",
    "            output_tokens = output.argmax(2)  # shape: (target_len, batch_size)\n",
    "\n",
    "            for i in range(target.shape[1]): \n",
    "                pred_tokens = output_tokens[1:, i]  # skip <sos>\n",
    "                trg_tokens = target[1:, i]\n",
    "\n",
    "                # Cut at <eos> \n",
    "                pred_sentence = []\n",
    "                for tok in pred_tokens:\n",
    "                    word = target_language.vocab.itos[tok.item()]\n",
    "                    if word == '<eos>':\n",
    "                        break\n",
    "                    pred_sentence.append(word)\n",
    "\n",
    "                trg_sentence = []\n",
    "                for tok in trg_tokens:\n",
    "                    word = target_language.vocab.itos[tok.item()]\n",
    "                    if word == '<eos>':\n",
    "                        break\n",
    "                    trg_sentence.append(word)\n",
    "\n",
    "                all_predictions.append(pred_sentence)\n",
    "                all_targets.append([trg_sentence])  # wrapped in list for BLEU\n",
    "\n",
    "            output_flat = output[1:].view(-1, output_dim)\n",
    "            target_flat = target[1:].view(-1)\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    bleu = bleu_score(all_predictions, all_targets) * 100\n",
    "    return epoch_loss / len(iterator), bleu\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1238b34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-thunder-7</strong> at: <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/dfaxkpvn' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/dfaxkpvn</a><br> View project at: <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250525_204148-dfaxkpvn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/denisangel2k2/Documents/Github/cnn-encoder/wandb/run-20250525_204217-ib0emdob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/ib0emdob' target=\"_blank\">true-capybara-8</a></strong> to <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/ib0emdob' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/nmt_training/runs/ib0emdob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (64x6 and 1024x2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[143], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iterator, val_iterator, optimizer, criterion, source_language, target_language, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mromanian\n\u001b[1;32m     11\u001b[0m source, target \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m  \u001b[38;5;66;03m# output shape (target_len, batch_size, output_size)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# target shape (target_len, batch_size)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#ignore the first token in target (sos)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[141], line 59\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     56\u001b[0m target_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(target_language\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m     58\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(target_len, batch_size, target_vocab_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 59\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# first token\u001b[39;00m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[141], line 128\u001b[0m, in \u001b[0;36mCnnEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m conv_output \u001b[38;5;241m=\u001b[39m conv_output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# shape (conv_output) = (batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_cell(conv_output)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# reshape for lstm\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (64x6 and 1024x2048)"
     ]
    }
   ],
   "source": [
    "train_model(model, train_iterator, val_iterator, optimizer, criterion, source_language, target_language, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f342dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: nu <unk>\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, model, source_field, target_field, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    if source_field.lower:\n",
    "        tokens = [token.lower() for token in source_field.tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = source_field.tokenize(sentence)\n",
    "\n",
    "    tokens = [source_field.init_token] + tokens + [source_field.eos_token]\n",
    "\n",
    "    # Convert tokens to numerical IDs, handling unknown tokens gracefully\n",
    "    src_indexes = [source_field.vocab.stoi.get(token, source_field.vocab.stoi[source_field.unk_token]) for token in tokens]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "    \n",
    "    trg_indexes = [target_field.vocab.stoi[target_field.init_token]]\n",
    "    \n",
    "    for _ in range(max_len):  # Use max_len parameter\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == target_field.vocab.stoi[target_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [target_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    # Exclude <sos> token at the beginning and <eos> token at the end if present\n",
    "    if trg_tokens and trg_tokens[0] == target_field.init_token:\n",
    "        trg_tokens = trg_tokens[1:]\n",
    "    if trg_tokens and trg_tokens[-1] == target_field.eos_token:\n",
    "        trg_tokens = trg_tokens[:-1]\n",
    "\n",
    "    return trg_tokens\n",
    "\n",
    "translated_sentence = translate_sentence(\n",
    "    \"I'm a man.\",\n",
    "    model,\n",
    "    source_language,\n",
    "    target_language,\n",
    "    device\n",
    ")\n",
    "print(\"Translated sentence:\", \" \".join(translated_sentence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
