{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76250373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.rescnn_bilstm import ResCnnBiLstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc310978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wmt16\", \"ro-en\")\n",
    "def map_language_data(data, lang):\n",
    "    items = []\n",
    "    for item in data:\n",
    "        items.append(item[lang])\n",
    "    return items\n",
    "\n",
    "train_data = {'en': map_language_data(dataset['train']['translation'], 'en'), 'ro': map_language_data(dataset['train']['translation'], 'ro')}\n",
    "val_data = {'en': map_language_data(dataset['validation']['translation'], 'en'), 'ro': map_language_data(dataset['validation']['translation'], 'ro')}\n",
    "test_data = {'en': map_language_data(dataset['test']['translation'], 'en'), 'ro': map_language_data(dataset['test']['translation'], 'ro')}\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_field, tgt_field):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_field = src_field\n",
    "        self.tgt_field = tgt_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_sentences[idx], self.tgt_sentences[idx]\n",
    "def collate_fn(batch):\n",
    "    src_sentences, tgt_sentences = zip(*batch)\n",
    "    src_tensor = SRC.process(src_sentences)\n",
    "    tgt_tensor = TGT.process(tgt_sentences)\n",
    "    return src_tensor, tgt_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c79279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size: 249\n",
      "Target vocab size: 266\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "SRC = Field(tokenize=get_tokenizer(\"spacy\", language=\"en_core_web_sm\"), init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n",
    "TGT = Field(tokenize=get_tokenizer(\"spacy\", language=\"ro_core_news_sm\"), init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n",
    "\n",
    "# Build vocab after defining the fields\n",
    "SRC.build_vocab(train_data['en'], min_freq=2)\n",
    "TGT.build_vocab(train_data['ro'], min_freq=2)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data['en'], train_data['ro'], SRC, TGT)\n",
    "val_dataset = TranslationDataset(val_data['en'], val_data['ro'], SRC, TGT)\n",
    "test_dataset = TranslationDataset(test_data['en'], test_data['ro'], SRC, TGT)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Source vocab size: {len(SRC.vocab)}\")\n",
    "print(f\"Target vocab size: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8595a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(reference_corpus, predicted_corpus):\n",
    "    \"\"\"\n",
    "    Computes average BLEU score over a batch of sentences.\n",
    "    Args:\n",
    "        reference_corpus: List of tokenized reference sentences.\n",
    "        predicted_corpus: List of tokenized predicted sentences.\n",
    "    Returns:\n",
    "        BLEU score (0 to 100)\n",
    "    \"\"\"\n",
    "    bleu_scores = []\n",
    "    for ref, pred in zip(reference_corpus, predicted_corpus):\n",
    "        score = sentence_bleu([ref], pred, weights=(0.5, 0.5))  \n",
    "        bleu_scores.append(score)\n",
    "    return sum(bleu_scores) / len(bleu_scores) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b23ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, tgt_vocab, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predicted_sentences = []\n",
    "    reference_sentences = []\n",
    "\n",
    "    tgt_itos = {i: tok for tok, i in tgt_vocab.stoi.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in val_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            output = model(src_batch, tgt_batch[:, :-1])\n",
    "            output_flat = output.reshape(-1, output.size(-1))\n",
    "            target = tgt_batch[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output_flat, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output.argmax(-1)  # (batch, tgt_len)\n",
    "            for pred, ref in zip(preds, tgt_batch):\n",
    "                pred_tokens = [tgt_itos[idx.item()] for idx in pred if idx.item() not in {tgt_vocab[\"<pad>\"], tgt_vocab[\"<eos>\"]}]\n",
    "                ref_tokens = [tgt_itos[idx.item()] for idx in ref if idx.item() not in {tgt_vocab[\"<pad>\"], tgt_vocab[\"<eos>\"]}]\n",
    "                predicted_sentences.append(pred_tokens)\n",
    "                reference_sentences.append(ref_tokens)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    bleu = compute_bleu(reference_sentences, predicted_sentences)\n",
    "    return avg_loss, bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7876642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "PAD_IDX = TGT.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, src_field, tgt_field,\n",
    "                device, num_epochs=10, clip=1.0):\n",
    "\n",
    "    wandb.init(project=\"cnn2rnn-translation\", config={\"epochs\": num_epochs})\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        for i, (src_batch, tgt_batch) in enumerate(train_loader):\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f\"Batch {i+1}/{len(train_loader)}\")\n",
    "            print(\"-\" * 20)\n",
    "            \n",
    "            output = model(src_batch, tgt_batch[:, :-1])  # teacher forcing\n",
    "\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "            print(f\"Target shape: {tgt_batch[:, 1:].shape}\")\n",
    "\n",
    "            output_contiguous = output.contiguous().to(device)\n",
    "            output_reshaped = output_contiguous.reshape(-1, output.size(-1))\n",
    "\n",
    "            target_contiguous = tgt_batch[:, 1:].contiguous().clone()\n",
    "            target_reshaped = target_contiguous.reshape(-1)\n",
    "\n",
    "            loss = criterion(output_reshaped, target_reshaped)\n",
    "            loss.backward()\n",
    "            print (f\"Loss: {loss.item()}\")\n",
    "            clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        val_loss, bleu = evaluate(model, val_loader, criterion, tgt_field, device)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"BLEU\": bleu\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}, BLEU = {bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95616899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ResCnnBiLstm(src_vocab_size=len(SRC.vocab),\n",
    "                        tgt_vocab_size=len(TGT.vocab))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e1ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdenismoldovan\u001b[0m (\u001b[33mdenismoldovan-babes-bolyai-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/denisangel2k2/Documents/Github/cnn-encoder/wandb/run-20250519_202728-h060qfcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/denismoldovan-babes-bolyai-university/cnn2rnn-translation/runs/h060qfcz' target=\"_blank\">electric-totem-18</a></strong> to <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/cnn2rnn-translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/cnn2rnn-translation' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/cnn2rnn-translation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/denismoldovan-babes-bolyai-university/cnn2rnn-translation/runs/h060qfcz' target=\"_blank\">https://wandb.ai/denismoldovan-babes-bolyai-university/cnn2rnn-translation/runs/h060qfcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "--------------------\n",
      "Batch 1/19073\n",
      "--------------------\n",
      "Output shape: torch.Size([32, 332, 266])\n",
      "Target shape: torch.Size([32, 332])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_model(model, train_loader, val_loader, optimizer, SRC, TGT, device=\"cpu\", num_epochs=10)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTGT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, src_field, tgt_field, device, num_epochs, clip)\u001b[0m\n\u001b[1;32m     39\u001b[0m target_reshaped \u001b[38;5;241m=\u001b[39m target_contiguous\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output_reshaped, target_reshaped)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[0;32m--> 624\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearning-env/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# train_model(model, train_loader, val_loader, optimizer, SRC, TGT, device=\"cpu\", num_epochs=10)\n",
    "train_model(model, train_loader, val_loader, optimizer, SRC, TGT, device=\"mps\" if torch.backends.mps.is_available() else \"cpu\", num_epochs=10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
